"""
Domain Clients for Unified Cloud Services

Convenience wrappers for accessing domain data across all services.
These clients provide domain-specific query patterns and are useful for:
- Analytics platforms that need to access multiple domains
- Cross-service quality gates
- Centralized data access patterns

All clients use StandardizedDomainCloudService under the hood.
"""

import pandas as pd
import logging
from datetime import datetime, timezone, timedelta
from typing import Any, Union
import re

from unified_cloud_services.domain.standardized_service import StandardizedDomainCloudService
from unified_cloud_services.core.cloud_config import CloudTarget
from unified_cloud_services.core.config import unified_config

logger = logging.getLogger(__name__)  # logging instance


class InstrumentsDomainClient:
    """
    Client for accessing instruments domain data.

    Provides convenience methods for querying instrument definitions with:
    - Date-based filtering
    - Venue/instrument type filtering
    - Symbol pattern matching
    - Multi-criteria queries
    """

    def __init__(
        self,
        project_id: str | None = None,
        gcs_bucket: str | None = None,
        bigquery_dataset: str | None = None,
    ):
        """
        Initialize instruments domain client.

        Args:
            project_id: GCP project ID (defaults to env var)
            gcs_bucket: GCS bucket name (defaults to env var)
            bigquery_dataset: BigQuery dataset (defaults to env var)
        """
        cloud_target = CloudTarget(
            project_id=project_id or unified_config.gcp_project_id,
            gcs_bucket=gcs_bucket or unified_config.instruments_gcs_bucket,
            bigquery_dataset=bigquery_dataset or unified_config.instruments_bigquery_dataset,
        )

        self.cloud_service = StandardizedDomainCloudService(
            domain="instruments", cloud_target=cloud_target
        )
        self.cloud_target = cloud_target

        logger.info(f"âœ… InstrumentsDomainClient initialized: bucket={cloud_target.gcs_bucket}")

    def get_instruments_for_date(
        self,
        date: str | datetime,
        venue: str | None = None,
        instrument_type: str | None = None,
        base_currency: str | None = None,
        quote_currency: str | None = None,
        symbol_pattern: str | None = None,
        instrument_ids: list[str] | None = None,
    ) -> pd.DataFrame:
        """
        Get instrument definitions for a specific date with filtering.

        Args:
            date: Date to get instruments for (YYYY-MM-DD string or datetime)
            venue: Filter by venue (BINANCE, DERIBIT, BYBIT, OKX, etc.)
            instrument_type: Filter by type (SPOT_PAIR, PERPETUAL, FUTURE, OPTION)
            base_currency: Filter by base asset (BTC, ETH, SOL, etc.)
            quote_currency: Filter by quote asset (USDT, USD, USDC, etc.)
            symbol_pattern: Regex pattern to match symbols
            instrument_ids: List of specific instrument IDs to include

        Returns:
            DataFrame with filtered instrument definitions
        """
        # Parse date
        if isinstance(date, str):
            date_obj = datetime.strptime(date, "%Y-%m-%d")
        else:
            date_obj = date

        date_str = date_obj.strftime("%Y-%m-%d")

        # Load instrument definitions from GCS
        gcs_path = f"instrument_availability/by_date/day-{date_str}/instruments.parquet"

        try:
            logger.info(f"ðŸ“¥ Loading instrument definitions for {date_str}")
            instruments_df = self.cloud_service.download_from_gcs(
                gcs_path=gcs_path, format="parquet"
            )

            if instruments_df.empty:
                logger.warning(f"âš ï¸ No instrument definitions found for {date_str}")
                return pd.DataFrame()

            # Filter by date availability
            instruments_df = self._filter_by_date_availability(instruments_df, date_obj)

            if instruments_df.empty:
                logger.warning(f"âš ï¸ No instruments available for {date_str} after date filtering")
                return pd.DataFrame()

            # Apply filters
            filtered_df = self._apply_filters(
                instruments_df,
                venue,
                instrument_type,
                base_currency,
                quote_currency,
                symbol_pattern,
                instrument_ids,
            )

            logger.info(f"ðŸ” Filtered to {len(filtered_df)} instruments")
            return filtered_df

        except Exception as e:
            logger.error(f"âŒ Failed to load instruments for {date_str}: {e}")
            return pd.DataFrame()

    def _filter_by_date_availability(self, df: pd.DataFrame, target_date: datetime) -> pd.DataFrame:
        """Filter instruments by date availability."""
        if df.empty:
            return df

        filtered_df = df.copy()

        # Filter by available_from_datetime
        if "available_from_datetime" in filtered_df.columns:

            def is_available_from(from_datetime_str):
                if pd.isna(from_datetime_str) or not from_datetime_str:
                    return True
                try:
                    from_date = datetime.fromisoformat(
                        str(from_datetime_str).replace("Z", "+00:00")
                    )
                    from_date = from_date.replace(tzinfo=None)
                    target_date_naive = (
                        target_date.replace(tzinfo=None) if target_date.tzinfo else target_date
                    )
                    return target_date_naive >= from_date
                except (ValueError, AttributeError):
                    return True

            filtered_df = filtered_df[
                filtered_df["available_from_datetime"].apply(is_available_from)
            ]

        # Filter by available_to_datetime
        if "available_to_datetime" in filtered_df.columns:

            def is_available_to(to_datetime_str):
                if pd.isna(to_datetime_str) or not to_datetime_str:
                    return True
                try:
                    to_date = datetime.fromisoformat(str(to_datetime_str).replace("Z", "+00:00"))
                    to_date = to_date.replace(tzinfo=None)
                    target_date_naive = (
                        target_date.replace(tzinfo=None) if target_date.tzinfo else target_date
                    )
                    return target_date_naive <= to_date
                except (ValueError, AttributeError):
                    return True

            filtered_df = filtered_df[filtered_df["available_to_datetime"].apply(is_available_to)]

        return filtered_df

    def _apply_filters(
        self,
        df: pd.DataFrame,
        venue: str | None = None,
        instrument_type: str | None = None,
        base_currency: str | None = None,
        quote_currency: str | None = None,
        symbol_pattern: str | None = None,
        instrument_ids: list[str] | None = None,
    ) -> pd.DataFrame:
        """Apply comprehensive filtering to instruments DataFrame"""

        if venue:
            venues = (
                [v.strip().upper() for v in venue.split(",")]
                if isinstance(venue, str)
                else [venue.upper()]
            )
            df = df.loc[df["venue"].isin(venues)]

        if instrument_type:
            types = (
                [t.strip().upper() for t in instrument_type.split(",")]
                if isinstance(instrument_type, str)
                else [instrument_type.upper()]
            )
            df = df.loc[df["instrument_type"].isin(types)]

        if base_currency:
            bases = (
                [b.strip().upper() for b in base_currency.split(",")]
                if isinstance(base_currency, str)
                else [base_currency.upper()]
            )
            df = df.loc[df["base_asset"].isin(bases)]

        if quote_currency:
            quotes = (
                [q.strip().upper() for q in quote_currency.split(",")]
                if isinstance(quote_currency, str)
                else [quote_currency.upper()]
            )
            df = df.loc[df["quote_asset"].isin(quotes)]

        if symbol_pattern:
            try:
                pattern = re.compile(symbol_pattern, re.IGNORECASE)
                df = df.loc[df["symbol"].str.match(pattern)]
            except re.error as e:
                logger.warning(f"âš ï¸ Invalid regex pattern '{symbol_pattern}': {e}")

        if instrument_ids:
            ids = (
                [i.strip() for i in instrument_ids.split(",")]
                if isinstance(instrument_ids, str)
                else instrument_ids
            )
            df = df.loc[df["instrument_key"].isin(ids)]

        return df

    def get_instruments_date_range(
        self,
        start_date: str | datetime,
        end_date: str | datetime,
        venue: str | None = None,
        instrument_type: str | None = None,
        **kwargs,
    ) -> pd.DataFrame:
        """
        Get instruments across a date range (union of all dates).

        Args:
            start_date: Start date
            end_date: End date
            venue: Optional venue filter
            instrument_type: Optional instrument type filter
            **kwargs: Additional filters passed to get_instruments_for_date

        Returns:
            DataFrame with unique instruments across date range
        """
        # Parse dates
        if isinstance(start_date, str):
            start_dt = datetime.strptime(start_date, "%Y-%m-%d")
        else:
            start_dt = start_date

        if isinstance(end_date, str):
            end_dt = datetime.strptime(end_date, "%Y-%m-%d")
        else:
            end_dt = end_date

        # Generate date range
        all_instruments = []
        current_date = start_dt
        while current_date <= end_dt:
            date_instruments = self.get_instruments_for_date(
                current_date, venue=venue, instrument_type=instrument_type, **kwargs
            )
            if not date_instruments.empty:
                date_instruments["query_date"] = current_date.strftime("%Y-%m-%d")
                all_instruments.append(date_instruments)
            current_date += timedelta(days=1)

        if not all_instruments:
            logger.warning(f"âš ï¸ No instruments found in date range {start_date} to {end_date}")
            return pd.DataFrame()

        # Combine and deduplicate by instrument_key
        combined_df = pd.concat(all_instruments, ignore_index=True)
        unique_df = combined_df.drop_duplicates(subset=["instrument_key"], keep="first")

        logger.info(f"ðŸ“Š Date range query: {len(unique_df)} unique instruments across date range")
        return unique_df

    def get_summary_stats(self, date: str | datetime) -> dict[str, Any]:
        """
        Get summary statistics for instruments on a specific date.

        Args:
            date: Date to analyze

        Returns:
            Dictionary with comprehensive statistics
        """
        instruments_df = self.get_instruments_for_date(date)

        if instruments_df.empty:
            return {"total_instruments": 0, "error": "No instruments found"}

        # Calculate statistics
        stats = {
            "total_instruments": len(instruments_df),
            "venues": instruments_df["venue"].nunique(),
            "venue_breakdown": instruments_df["venue"].value_counts().to_dict(),
            "instrument_types": instruments_df["instrument_type"].nunique(),
            "type_breakdown": instruments_df["instrument_type"].value_counts().to_dict(),
            "base_currencies": instruments_df["base_asset"].nunique(),
            "quote_currencies": instruments_df["quote_asset"].nunique(),
            "top_base_currencies": instruments_df["base_asset"].value_counts().head(10).to_dict(),
            "top_quote_currencies": instruments_df["quote_asset"].value_counts().head(10).to_dict(),
        }

        if "ccxt_symbol" in instruments_df.columns:
            stats["ccxt_coverage"] = {
                "instruments_with_ccxt": len(instruments_df[instruments_df["ccxt_symbol"] != ""]),
                "ccxt_coverage_percent": len(instruments_df[instruments_df["ccxt_symbol"] != ""])
                / len(instruments_df)
                * 100,
            }

        if "data_types" in instruments_df.columns:
            stats["data_type_coverage"] = {
                "trades": len(
                    instruments_df[instruments_df["data_types"].str.contains("trades", na=False)]
                ),
                "book_snapshot_5": len(
                    instruments_df[
                        instruments_df["data_types"].str.contains("book_snapshot_5", na=False)
                    ]
                ),
                "derivative_ticker": len(
                    instruments_df[
                        instruments_df["data_types"].str.contains("derivative_ticker", na=False)
                    ]
                ),
                "liquidations": len(
                    instruments_df[
                        instruments_df["data_types"].str.contains("liquidations", na=False)
                    ]
                ),
                "options_chain": len(
                    instruments_df[
                        instruments_df["data_types"].str.contains("options_chain", na=False)
                    ]
                ),
            }

        logger.info(
            f"ðŸ“Š Generated summary stats for {date}: {stats['total_instruments']} instruments"
        )
        return stats

    def get_instrument_details(
        self, date: str | datetime, instrument_id: str
    ) -> dict[str, Any] | None:
        """
        Get detailed information for a specific instrument ID.

        Args:
            date: Date to check
            instrument_id: Canonical instrument ID

        Returns:
            Dictionary with instrument details or None if not found
        """
        instruments_df = self.get_instruments_for_date(date, instrument_ids=[instrument_id])

        if instruments_df.empty:
            logger.warning(f"âš ï¸ Instrument not found: {instrument_id} on {date}")
            return None

        # Convert to dictionary
        instrument_data = instruments_df.iloc[0].to_dict()
        logger.info(f"âœ… Found instrument details for {instrument_id}")
        return instrument_data

    def get_trading_parameters(
        self, date: str | datetime, instrument_id: str
    ) -> dict[str, Any] | None:
        """
        Get trading parameters for an instrument (tick_size, min_size, etc.).

        Args:
            date: Date to check
            instrument_id: Canonical instrument ID

        Returns:
            Dictionary with trading parameters or None if not found
        """
        instrument = self.get_instrument_details(date, instrument_id)
        if not instrument:
            return None

        trading_params = {
            "tick_size": instrument.get("tick_size", ""),
            "min_size": instrument.get("min_size", ""),
            "contract_size": instrument.get("contract_size"),
            "ccxt_symbol": instrument.get("ccxt_symbol", ""),
            "ccxt_exchange": instrument.get("ccxt_exchange", ""),
            "inverse": instrument.get("inverse", False),
            "data_types": (
                instrument.get("data_types", "").split(",") if instrument.get("data_types") else []
            ),
        }

        logger.info(f"ðŸ“Š Trading parameters for {instrument_id}: {len(trading_params)} fields")
        return trading_params

    def get_instruments_by_data_type(
        self,
        date: str | datetime,
        data_type: str,
        venue: str | None = None,
        limit: int = 1000,
    ) -> pd.DataFrame:
        """
        Get instruments that support a specific data type.

        Args:
            date: Date to check
            data_type: Data type to filter by (trades, book_snapshot_5, derivative_ticker, etc.)
            venue: Optional venue filter
            limit: Maximum results to return

        Returns:
            DataFrame with instruments supporting the data type
        """
        instruments_df = self.get_instruments_for_date(date, venue=venue)

        if instruments_df.empty:
            return pd.DataFrame()

        # Filter by data type availability
        if "data_types" in instruments_df.columns:

            def has_data_type(data_types_str):
                if not data_types_str:
                    return False
                available_types = [dt.strip() for dt in data_types_str.split(",")]
                return data_type in available_types

            filtered_df = instruments_df[instruments_df["data_types"].apply(has_data_type)]
        else:
            filtered_df = pd.DataFrame()

        if len(filtered_df) > limit:
            logger.info(f"ðŸ” Limiting results to {limit} instruments (found {len(filtered_df)})")
            filtered_df = filtered_df.head(limit)

        logger.info(f"ðŸ“Š Found {len(filtered_df)} instruments with {data_type} data")
        return filtered_df

    def search_instruments_by_symbol(
        self,
        date: str | datetime,
        symbol_pattern: str,
        venue: str | None = None,
        limit: int = 100,
    ) -> pd.DataFrame:
        """
        Search instruments by symbol pattern (regex supported).

        Args:
            date: Date to search
            symbol_pattern: Pattern to match (supports regex)
            venue: Optional venue filter
            limit: Maximum results to return

        Returns:
            DataFrame with matching instruments
        """
        instruments_df = self.get_instruments_for_date(
            date, venue=venue, symbol_pattern=symbol_pattern
        )

        if len(instruments_df) > limit:
            logger.info(f"ðŸ” Limiting results to {limit} instruments (found {len(instruments_df)})")
            instruments_df = instruments_df.head(limit)

        return instruments_df

    def get_expiring_instruments(
        self,
        date: str | datetime,
        days_until_expiry: int = 30,
        instrument_type: str | None = None,
    ) -> pd.DataFrame:
        """
        Get instruments expiring within specified days.

        Args:
            date: Reference date
            days_until_expiry: Number of days to look ahead
            instrument_type: Optional filter (FUTURE, OPTION)

        Returns:
            DataFrame with expiring instruments
        """
        instruments_df = self.get_instruments_for_date(date, instrument_type=instrument_type)

        if instruments_df.empty:
            return pd.DataFrame()

        # Filter instruments with expiry dates
        if "available_to_datetime" not in instruments_df.columns:
            return pd.DataFrame()

        expiring_df = instruments_df[instruments_df["available_to_datetime"].notna()]

        if expiring_df.empty:
            return pd.DataFrame()

        # Calculate expiry dates
        if isinstance(date, str):
            ref_date = datetime.strptime(date, "%Y-%m-%d")
        else:
            ref_date = date

        cutoff_date = ref_date + timedelta(days=days_until_expiry)

        def is_expiring_soon(expiry_str):
            if not expiry_str:
                return False
            try:
                expiry_dt = datetime.fromisoformat(str(expiry_str).replace("Z", "+00:00"))
                return ref_date <= expiry_dt.replace(tzinfo=None) <= cutoff_date
            except:
                return False

        expiring_df = expiring_df[expiring_df["available_to_datetime"].apply(is_expiring_soon)]

        logger.info(
            f"ðŸ“Š Found {len(expiring_df)} instruments expiring within {days_until_expiry} days"
        )
        return expiring_df


class MarketCandleDataDomainClient:
    """
    Client for accessing processed candle data from market-data-processing-service.

    Provides convenience methods for querying:
    - Processed candles (15s, 1m, 5m, 15m, 1h, etc.)
    - Multiple data types (trades, book_snapshot_5, liquidations, etc.)
    """

    def __init__(
        self,
        project_id: str | None = None,
        gcs_bucket: str | None = None,
        bigquery_dataset: str | None = None,
    ):
        """
        Initialize market candle data domain client.

        Args:
            project_id: GCP project ID (defaults to env var)
            gcs_bucket: GCS bucket name (defaults to env var)
            bigquery_dataset: BigQuery dataset (defaults to env var)
        """
        cloud_target = CloudTarget(
            project_id=project_id or unified_config.gcp_project_id,
            gcs_bucket=gcs_bucket or unified_config.market_data_gcs_bucket,
            bigquery_dataset=bigquery_dataset or unified_config.market_data_bigquery_dataset,
        )

        self.cloud_service = StandardizedDomainCloudService(
            domain="market_data", cloud_target=cloud_target
        )
        self.cloud_target = cloud_target

        logger.info(
            f"âœ… MarketCandleDataDomainClient initialized: bucket={cloud_target.gcs_bucket}"
        )

    def get_candles(
        self,
        date: datetime,
        instrument_id: str,
        timeframe: str = "15s",
        data_type: str = "trades",
        venue: str | None = None,
    ) -> pd.DataFrame:
        """
        Get processed candles for a specific date and instrument.

        Args:
            date: Target date
            instrument_id: Instrument ID (e.g., 'BINANCE-FUTURES:PERPETUAL:BTC-USDT')
            timeframe: Candle timeframe (e.g., '15s', '1m', '5m', '1h')
            data_type: Data type (e.g., 'trades', 'book_snapshot_5')
            venue: Optional venue filter

        Returns:
            DataFrame with candles
        """
        date_str = date.strftime("%Y-%m-%d")

        # Build GCS path
        if venue:
            gcs_path = f"processed_candles/by_date/day-{date_str}/timeframe-{timeframe}/data_type-{data_type}/{venue}/{instrument_id}.parquet"
        else:
            gcs_path = f"processed_candles/by_date/day-{date_str}/timeframe-{timeframe}/data_type-{data_type}/{instrument_id}.parquet"

        try:
            logger.info(f"ðŸ“¥ Loading candles: {gcs_path}")
            candles_df = self.cloud_service.download_from_gcs(gcs_path=gcs_path, format="parquet")

            if candles_df.empty:
                logger.warning(f"âš ï¸ No candles found for {instrument_id} on {date_str}")
            else:
                logger.info(f"âœ… Loaded {len(candles_df)} candles")

            return candles_df

        except Exception as e:
            logger.error(f"âŒ Failed to load candles: {e}")
            return pd.DataFrame()

    def get_candles_range(
        self,
        start_date: datetime,
        end_date: datetime,
        instrument_id: str,
        timeframe: str = "15s",
        data_type: str = "trades",
        venue: str | None = None,
    ) -> pd.DataFrame:
        """
        Get processed candles for a date range.

        Args:
            start_date: Start date
            end_date: End date
            instrument_id: Instrument ID
            timeframe: Candle timeframe
            data_type: Data type
            venue: Optional venue filter

        Returns:
            Combined DataFrame with candles for all dates
        """
        all_candles = []
        current_date = start_date

        while current_date <= end_date:
            candles = self.get_candles(
                date=current_date,
                instrument_id=instrument_id,
                timeframe=timeframe,
                data_type=data_type,
                venue=venue,
            )

            if not candles.empty:
                all_candles.append(candles)

            # Move to next day
            current_date = current_date.replace(hour=0, minute=0, second=0, microsecond=0)
            current_date = current_date.replace(tzinfo=timezone.utc) + pd.Timedelta(days=1)

        if all_candles:
            return pd.concat(all_candles, ignore_index=True)
        else:
            return pd.DataFrame()


class MarketTickDataDomainClient:
    """
    Client for accessing raw tick data from market-tick-data-handler.

    Provides convenience methods for querying:
    - Raw tick data (trades, book_snapshot_5, liquidations, etc.)
    - Tick data by hour
    - Tick data by date range
    """

    def __init__(
        self,
        project_id: str | None = None,
        gcs_bucket: str | None = None,
        bigquery_dataset: str | None = None,
    ):
        """
        Initialize market tick data domain client.

        Args:
            project_id: GCP project ID (defaults to env var)
            gcs_bucket: GCS bucket name (defaults to env var)
            bigquery_dataset: BigQuery dataset (defaults to env var)
        """
        cloud_target = CloudTarget(
            project_id=project_id or unified_config.gcp_project_id,
            gcs_bucket=gcs_bucket or unified_config.market_data_gcs_bucket,
            bigquery_dataset=bigquery_dataset or unified_config.market_data_bigquery_dataset,
        )

        self.cloud_service = StandardizedDomainCloudService(
            domain="market_data", cloud_target=cloud_target
        )
        self.cloud_target = cloud_target

        logger.info(f"âœ… MarketTickDataDomainClient initialized: bucket={cloud_target.gcs_bucket}")

    def get_tick_data(
        self,
        date: datetime,
        instrument_id: str,
        data_type: str = "trades",
        hour: int | None = None,
        venue: str | None = None,
    ) -> pd.DataFrame:
        """
        Get raw tick data for a specific date and instrument.

        Args:
            date: Target date
            instrument_id: Instrument ID (e.g., 'BINANCE-FUTURES:PERPETUAL:BTC-USDT')
            data_type: Data type (e.g., 'trades', 'book_snapshot_5', 'liquidations')
            hour: Optional hour filter (0-23)
            venue: Optional venue filter

        Returns:
            DataFrame with tick data
        """
        date_str = date.strftime("%Y-%m-%d")

        # Build GCS path
        # Path format: raw_tick_data/by_date/day-{YYYY-MM-DD}/data_type-{data_type}/{instrument_id}.parquet
        if hour is not None:
            hour_str = f"{hour:02d}"
            if venue:
                gcs_path = f"raw_tick_data/by_date/day-{date_str}/data_type-{data_type}/hour-{hour_str}/{venue}/{instrument_id}.parquet"
            else:
                gcs_path = f"raw_tick_data/by_date/day-{date_str}/data_type-{data_type}/hour-{hour_str}/{instrument_id}.parquet"
        else:
            if venue:
                gcs_path = f"raw_tick_data/by_date/day-{date_str}/data_type-{data_type}/{venue}/{instrument_id}.parquet"
            else:
                gcs_path = f"raw_tick_data/by_date/day-{date_str}/data_type-{data_type}/{instrument_id}.parquet"

        try:
            logger.info(f"ðŸ“¥ Loading tick data: {gcs_path}")
            tick_df = self.cloud_service.download_from_gcs(gcs_path=gcs_path, format="parquet")

            if tick_df.empty:
                logger.warning(f"âš ï¸ No tick data found for {instrument_id} on {date_str}")
            else:
                logger.info(f"âœ… Loaded {len(tick_df)} tick records")

            return tick_df

        except Exception as e:
            logger.error(f"âŒ Failed to load tick data: {e}")
            return pd.DataFrame()

    def get_tick_data_range(
        self,
        start_date: datetime,
        end_date: datetime,
        instrument_id: str,
        data_type: str = "trades",
        venue: str | None = None,
    ) -> pd.DataFrame:
        """
        Get raw tick data for a date range.

        Args:
            start_date: Start date
            end_date: End date
            instrument_id: Instrument ID
            data_type: Data type
            venue: Optional venue filter

        Returns:
            Combined DataFrame with tick data for all dates
        """
        all_ticks = []
        current_date = start_date

        while current_date <= end_date:
            ticks = self.get_tick_data(
                date=current_date,
                instrument_id=instrument_id,
                data_type=data_type,
                venue=venue,
            )

            if not ticks.empty:
                all_ticks.append(ticks)

            # Move to next day
            current_date = current_date.replace(hour=0, minute=0, second=0, microsecond=0)
            current_date = current_date.replace(tzinfo=timezone.utc) + pd.Timedelta(days=1)

        if all_ticks:
            return pd.concat(all_ticks, ignore_index=True)
        else:
            return pd.DataFrame()


# Deprecated: Keep for backward compatibility
class MarketDataDomainClient(MarketCandleDataDomainClient):
    """
    âš ï¸ DEPRECATED: Use MarketCandleDataDomainClient or MarketTickDataDomainClient instead.

    This class is kept for backward compatibility only.
    """

    def __init__(self, *args, **kwargs):
        import warnings

        warnings.warn(
            "MarketDataDomainClient is deprecated. Use MarketCandleDataDomainClient or MarketTickDataDomainClient instead. "
            "See docs/CLIENTS_DEPRECATION_GUIDE.md for migration details.",
            DeprecationWarning,
            stacklevel=2,
        )
        super().__init__(*args, **kwargs)


class FeaturesDomainClient:
    """
    Client for accessing features domain data.

    Provides convenience methods for querying:
    - Delta-one features
    - Volatility features
    - On-chain features
    - Calendar features
    """

    def __init__(
        self,
        project_id: str | None = None,
        gcs_bucket: str | None = None,
        bigquery_dataset: str | None = None,
        feature_type: str = "delta_one",  # 'delta_one', 'volatility', 'onchain', 'calendar'
    ):
        """
        Initialize features domain client.

        Args:
            project_id: GCP project ID (defaults to env var)
            gcs_bucket: GCS bucket name (defaults to env var)
            bigquery_dataset: BigQuery dataset (defaults to env var)
            feature_type: Type of features ('delta_one', 'volatility', 'onchain', 'calendar')
        """
        # Map feature types to datasets
        dataset_map = {
            "delta_one": unified_config.features_bigquery_dataset,
            "volatility": unified_config.volatility_features_bigquery_dataset,
            "onchain": unified_config.onchain_features_bigquery_dataset,
            "calendar": unified_config.calendar_features_bigquery_dataset,
        }

        cloud_target = CloudTarget(
            project_id=project_id or unified_config.gcp_project_id,
            gcs_bucket=gcs_bucket or unified_config.features_gcs_bucket,
            bigquery_dataset=bigquery_dataset
            or dataset_map.get(feature_type, unified_config.features_bigquery_dataset),
        )

        self.cloud_service = StandardizedDomainCloudService(
            domain="features", cloud_target=cloud_target
        )
        self.cloud_target = cloud_target
        self.feature_type = feature_type

        logger.info(
            f"âœ… FeaturesDomainClient initialized: bucket={cloud_target.gcs_bucket}, type={feature_type}"
        )

    def get_features(
        self, date: datetime, instrument_id: str, feature_set: str | None = None
    ) -> pd.DataFrame:
        """
        Get features for a specific date and instrument.

        Args:
            date: Target date
            instrument_id: Instrument ID
            feature_set: Optional feature set filter

        Returns:
            DataFrame with features
        """
        date_str = date.strftime("%Y-%m-%d")

        # Build GCS path based on feature type
        if feature_set:
            gcs_path = f"features/{self.feature_type}/by_date/day-{date_str}/feature_set-{feature_set}/{instrument_id}.parquet"
        else:
            gcs_path = (
                f"features/{self.feature_type}/by_date/day-{date_str}/{instrument_id}.parquet"
            )

        try:
            logger.info(f"ðŸ“¥ Loading {self.feature_type} features: {gcs_path}")
            features_df = self.cloud_service.download_from_gcs(gcs_path=gcs_path, format="parquet")

            if features_df.empty:
                logger.warning(f"âš ï¸ No features found for {instrument_id} on {date_str}")
            else:
                logger.info(f"âœ… Loaded {len(features_df)} feature rows")

            return features_df

        except Exception as e:
            logger.error(f"âŒ Failed to load features: {e}")
            return pd.DataFrame()


class ExecutionDomainClient:
    """
    Client for accessing execution domain data (backtest results).

    Provides methods for:
    - Loading backtest summaries
    - Loading fills, orders, positions
    - Loading equity curves with byte-range streaming
    - Listing available backtest runs
    """

    def __init__(
        self,
        project_id: str | None = None,
        gcs_bucket: str | None = None,
        bigquery_dataset: str | None = None,
    ):
        """
        Initialize execution domain client.

        Args:
            project_id: GCP project ID (defaults to env var)
            gcs_bucket: GCS bucket name (defaults to env var or 'execution-store-cefi-central-element-323112')
            bigquery_dataset: BigQuery dataset (defaults to env var or 'execution')
        """
        cloud_target = CloudTarget(
            project_id=project_id or unified_config.gcp_project_id,
            gcs_bucket=gcs_bucket or getattr(unified_config, 'execution_gcs_bucket', 'execution-store-cefi-central-element-323112'),
            bigquery_dataset=bigquery_dataset or getattr(unified_config, 'execution_bigquery_dataset', 'execution'),
        )

        self.cloud_service = StandardizedDomainCloudService(
            domain="execution", cloud_target=cloud_target
        )
        self.cloud_target = cloud_target

        logger.info(f"âœ… ExecutionDomainClient initialized: bucket={cloud_target.gcs_bucket}")

    def get_backtest_summary(self, run_id: str) -> dict:
        """
        Load backtest summary JSON.

        Args:
            run_id: Backtest run ID (e.g., 'BT-20231223-001')

        Returns:
            Summary dict with pnl, metrics, execution stats
        """
        gcs_path = f"backtest_results/{run_id}/summary.json"

        try:
            logger.info(f"ðŸ“¥ Loading backtest summary: {gcs_path}")
            summary = self.cloud_service.download_from_gcs(gcs_path=gcs_path, format="json")
            return summary

        except Exception as e:
            logger.error(f"âŒ Failed to load backtest summary: {e}")
            return {}

    def get_backtest_fills(self, run_id: str) -> pd.DataFrame:
        """
        Load all fills/trades from a backtest run.

        Args:
            run_id: Backtest run ID

        Returns:
            DataFrame with fill records
        """
        gcs_path = f"backtest_results/{run_id}/fills.parquet"

        try:
            logger.info(f"ðŸ“¥ Loading backtest fills: {gcs_path}")
            fills = self.cloud_service.download_from_gcs(gcs_path=gcs_path, format="parquet")

            if fills.empty:
                logger.warning(f"âš ï¸ No fills found for run {run_id}")
            else:
                logger.info(f"âœ… Loaded {len(fills)} fills")

            return fills

        except Exception as e:
            logger.error(f"âŒ Failed to load fills: {e}")
            return pd.DataFrame()

    def get_backtest_orders(self, run_id: str) -> pd.DataFrame:
        """
        Load all orders from a backtest run.

        Args:
            run_id: Backtest run ID

        Returns:
            DataFrame with order records
        """
        gcs_path = f"backtest_results/{run_id}/orders.parquet"

        try:
            logger.info(f"ðŸ“¥ Loading backtest orders: {gcs_path}")
            orders = self.cloud_service.download_from_gcs(gcs_path=gcs_path, format="parquet")

            if orders.empty:
                logger.warning(f"âš ï¸ No orders found for run {run_id}")
            else:
                logger.info(f"âœ… Loaded {len(orders)} orders")

            return orders

        except Exception as e:
            logger.error(f"âŒ Failed to load orders: {e}")
            return pd.DataFrame()

    def get_backtest_positions(self, run_id: str) -> pd.DataFrame:
        """
        Load position timeline from a backtest run.

        Args:
            run_id: Backtest run ID

        Returns:
            DataFrame with position snapshots
        """
        gcs_path = f"backtest_results/{run_id}/positions.parquet"

        try:
            logger.info(f"ðŸ“¥ Loading backtest positions: {gcs_path}")
            positions = self.cloud_service.download_from_gcs(gcs_path=gcs_path, format="parquet")

            if positions.empty:
                logger.warning(f"âš ï¸ No positions found for run {run_id}")
            else:
                logger.info(f"âœ… Loaded {len(positions)} position snapshots")

            return positions

        except Exception as e:
            logger.error(f"âŒ Failed to load positions: {e}")
            return pd.DataFrame()

    def get_equity_curve(
        self,
        run_id: str,
        start_time: datetime | None = None,
        end_time: datetime | None = None,
    ) -> pd.DataFrame:
        """
        Load equity curve with optional time filtering (byte-range streaming).

        Args:
            run_id: Backtest run ID
            start_time: Optional start time for filtering
            end_time: Optional end time for filtering

        Returns:
            DataFrame with equity curve data
        """
        gcs_path = f"backtest_results/{run_id}/equity_curve.parquet"

        try:
            if start_time and end_time:
                logger.info(f"ðŸ“¥ Streaming equity curve: {gcs_path} ({start_time} to {end_time})")
                equity = self.cloud_service.download_from_gcs_streaming(
                    gcs_path=gcs_path,
                    timestamp_range=(start_time, end_time),
                    timestamp_column="ts_event",
                    use_byte_range=True,
                )
            else:
                logger.info(f"ðŸ“¥ Loading full equity curve: {gcs_path}")
                equity = self.cloud_service.download_from_gcs(gcs_path=gcs_path, format="parquet")

            if equity.empty:
                logger.warning(f"âš ï¸ No equity data found for run {run_id}")
            else:
                logger.info(f"âœ… Loaded {len(equity)} equity snapshots")

            return equity

        except Exception as e:
            logger.error(f"âŒ Failed to load equity curve: {e}")
            return pd.DataFrame()

    def list_backtest_runs(self, prefix: str = "") -> list[str]:
        """
        List available backtest run IDs.

        Args:
            prefix: Optional prefix filter (e.g., 'BT-2023')

        Returns:
            List of run IDs
        """
        try:
            from google.cloud import storage

            logger.info(f"ðŸ“‹ Listing backtest runs (prefix='{prefix}')")
            
            client = storage.Client()
            bucket = client.bucket(self.cloud_target.gcs_bucket)
            blobs = bucket.list_blobs(prefix=f"backtest_results/{prefix}")

            # Extract unique run IDs
            run_ids = set()
            for blob in blobs:
                parts = blob.name.replace("backtest_results/", "").split("/")
                if parts and parts[0]:
                    run_ids.add(parts[0])

            run_ids_list = sorted(list(run_ids))
            logger.info(f"âœ… Found {len(run_ids_list)} backtest runs")
            return run_ids_list

        except Exception as e:
            logger.error(f"âŒ Failed to list backtest runs: {e}")
            return []


# Factory functions for creating domain clients
def create_instruments_client(**kwargs) -> InstrumentsDomainClient:
    """Factory function to create InstrumentsDomainClient."""
    return InstrumentsDomainClient(**kwargs)


def create_market_candle_data_client(**kwargs) -> MarketCandleDataDomainClient:
    """Factory function to create MarketCandleDataDomainClient."""
    return MarketCandleDataDomainClient(**kwargs)


def create_market_tick_data_client(**kwargs) -> MarketTickDataDomainClient:
    """Factory function to create MarketTickDataDomainClient."""
    return MarketTickDataDomainClient(**kwargs)


def create_execution_client(**kwargs) -> ExecutionDomainClient:
    """Factory function to create ExecutionDomainClient."""
    return ExecutionDomainClient(**kwargs)


# Deprecated: Keep for backward compatibility
def create_market_data_client(**kwargs) -> MarketDataDomainClient:
    """
    âš ï¸ DEPRECATED: Use create_market_candle_data_client() or create_market_tick_data_client() instead.

    Factory function to create MarketDataDomainClient (deprecated).
    """
    import warnings

    warnings.warn(
        "create_market_data_client() is deprecated. Use create_market_candle_data_client() or create_market_tick_data_client() instead. "
        "See docs/CLIENTS_DEPRECATION_GUIDE.md for migration details.",
        DeprecationWarning,
        stacklevel=2,
    )
    return MarketDataDomainClient(**kwargs)


def create_features_client(feature_type: str = "delta_one", **kwargs) -> FeaturesDomainClient:
    """Factory function to create FeaturesDomainClient."""
    return FeaturesDomainClient(feature_type=feature_type, **kwargs)
